---
title: "EIO-LCA Analysis: Pet Food Supply Chain"
author: "Weiquan Luo, Mingjun Ma"
date: "`r Sys.Date()`"
output: 
  github_document:
      toc: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```


# Background information
According to the APPA (American Pet Product Association) 2019-2020 pet owner survey, around 84.9 million U.S. households own pet, which is about 67% of the U.S. homes (APPA, 2019). In 2018, the U.S. customer spent 72.56 billion dollars on their pets, of which 30.32 billion dollars is for pet food. From a 2017 U.S. pet owners survey, 2% of dog and cat owners select their pet food based on product claim of sustainable or eco-friendly pet food formula. Although the percentage is low, the interest in the sustainability of the pet food has grown a lot recently. 

# Interesting question

The goal of this project is to study the environmental impact of a certain amount of production with Economic Input-Output Life Cycle Assessment (EIO-LCA) method, which estimates activities in our economy in the materials and energy resources required for and the environmental impact resulting from. The environmental impacts involove conventional air poluten (CAP), greenhouse gass (GHG), and toix release (TOX). Cradle-to-grave is the full Life Cycle Assessment from resource extraction to use phase and disposal phase. Specificallym, this analysis is base on the Cradle-to-grave EIO-LCA result to further understand how all industrial stages of producing Million Dollars product *in Dog and Cat Food Manufacturing* (code 311111 in NAICS 2002) are different in environmental impact. The study aim to answer the following questions:

1.  which industry(s) have larger impact among all industries?
2.  what are the relationship between some impact relative to the input (i.e. Energy, water withdraw)?
3.  how the outlier industry(s) behave in linear regression models


# Hightlight of result

* for dog and cat food Manufacturing, the greatest environmental impacts come frome any raw material production industries such as agricultural farming.
* most of industries use either NonFossoil Eletrecity or Fossoil Eletrecity
* for those industries using biowaste as energy source have higher impact in toxic.

# Data description
The dataset for this project is the first pass life cycle assessment results for cat and dog food manufacturing. It provides the environmental impact information into the pet food supply chain. The LCA data was generated through EIO-LCA website(http://www.eiolca.net/cgi-bin/dft/use.pl).  The model for getting the LCA data is US 2002 producer price benchmark. In order to get the dog and cat food manufacturing LCA data, the user needs to select “Food beverage and tobacco” sector group, dog and cat food manufacturing sector and then the amount of economic activity for this sector(e.g. 1 millon dollars). After setting up the sector and economic parameters, The user could select different economical and environmental impact to get the LCA results.
The LCA results are ready for downloading as excel files. The raw data was lacking the columns name for different impact feature and index for identifying the different sectors and sector group. The web scraping is necessary for the columns name and NAICS sector code. 

# Workflow

<center>
![EIO-LCA:Pet Food Supply Chain](img/EIO-LCA_ Pet Food Supply Chain.png){width=100%}
</center>


```{r pkg, include=FALSE}
library(dplyr)
library(purrr)
library(kableExtra)
library(plotly)
library(RColorBrewer)
library(ggplot2)
```

# Explore the data

After webscaping, combinding the raw data, and manually making minor modification, we result a datafarme stored as 'dat_311111_1M_v2.csv.
 
```{r read_csv}
# data input
dat <- read.csv("data/dat_311111_1M_v2.csv")
dim(dat)
```

```{r clearData}
# calculate and rename
calculate_formula_replace_nm <- function(data, formula = y~1000*x, pattern= pattern, replacement= replacement){
  
  calculate_formula<- function(data, formula = formula){
    as.function <- function(formula) {
      cmd <- tail(as.character(formula),1)
      exp <- parse(text=cmd)
      function(...) eval(exp, list(...))
    }
    formula.function <- as.function(formula)
    result<- formula.function(x=data)
    return(result)
  }
  data <- data %>% dplyr::mutate_all(calculate_formula, formula = formula) %>% 
    stats::setNames(stringr::str_replace_all(names(.), pattern= pattern, replacement= replacement )) 
  return(data)
}
# piping: Sort environment impact group and input resouce, unit convertion to result no 0<.<1, rename by unit
CPA <- dat %>% select(CO.t, NH3.t, NOx.t, PM10.t, PM2.5.t, SO2.t, VOC.t) %>% 
  calculate_formula_replace_nm(formula = y~x*10^6, pattern= "\\.t", replacement= ".g")
GHG <- dat %>% select(Total.t.CO2e, CO2.Fossil.t.CO2e, CO2.Process.t.CO2e, CH4.t.CO2e, HFC.PFCs.t.CO2e) %>% 
  calculate_formula_replace_nm(formula = y~x*10^6, pattern= "\\.t", replacement= ".g")
TOX <- dat %>% select(Fugitive.kg, Stack.kg, Total.Air.kg, Surface.water.kg, U_ground.Water.kg, Land.kg, Offiste.kg, POTW.Metal.kg) %>% 
  calculate_formula_replace_nm(formula = y~x*10^6, pattern= "\\.kg", replacement= ".mg")
resource <- dat %>% select(Coal.TJ, NatGase.TJ, Petrol.TJ,Bio.Waste.TJ, NonFossElec.TJ, Water.Withdrawals.Kgal) %>% 
  calculate_formula_replace_nm(formula = y~x*10^6, pattern= "\\.TJ", replacement= ".MJ")
ID <- dat %>% select(Sector, Description, name_sub, Sector_sub) %>% mutate_all(as.factor)
dat <- cbind(ID, CPA, GHG, TOX, resource)

# summary statistic for X variable
ys <- cbind(CPA, GHG, TOX)
psych::describe(ys) %>% knitr::kable(format = "markdown") 
```

## Correlation

Observation from correlation plot

* The scatter plots show most information are concentrate at the origin of the feature space, but sparse anywhere else (heteroskedastic);
* The distribution plots show variables are highly skewed (not normally distributed);
* The spearman correlations show that most independent variables are moderate correlative, but some are unlikly and some are likly correlative (multicollinearity).

```{r corplot}
# raw X variable
psych::pairs.panels(resource, method = "spearman")
```

The data is suitable for log transformation and could be modelled in LogLog transformation. 

con

* the interpretation of each parameter distorted from scalar to ratio of the scalar quantity (i.e. For regressions, the coefficent is interpret as the reletive change, in clusterings, the cluster is the group of same magnitude).
* it is not able to fix the issue of multicollinearity, so variable/model selection are required.

pro

* the information at the origin can be stretched out.  
* the resulting linear regression can be interpreted at elastisity (similar to the idea cobb-douglas utility function in Econometrics)

```{r corplot_LN}
# Ln X variable
psych::pairs.panels(log(resource+1), method = "spearman")
```


```{r GHG_TotalCO2, eval=FALSE, include=FALSE}
# Define the number of colors you want
nb.cols <- dat$Sector %>% unique() %>% length()
mycolors <- colorRampPalette(brewer.pal(8, "Set2"))(nb.cols)

# calculate with formula only
calculate_formula <- function(data, formula = y~1000*x){
  as.function <- function(formula) {
    cmd <- tail(as.character(formula),1)
    exp <- parse(text=cmd)
    function(...) eval(exp, list(...))
  }
  formula.function <- as.function(formula)
  result<- formula.function(x=data)
  return(result)
}

X <- resource %>% 
  select(Coal.MJ, Petrol.MJ, NatGase.MJ) %>% 
  mutate_all(calculate_formula, formula = y~x+1)
y <- GHG %>% select(Total.g.CO2e)
dat_GHG_TotalCO2 <- cbind(ID, y, X)

p <- plot_ly(dat_GHG_TotalCO2, 
             x = ~Coal.MJ, 
             y = ~Petrol.MJ, 
             z = ~NatGase.MJ,
             hovertemplate = paste('<b>%{text}</b>',
                                   '<br>Coal.MJ: %{x}',
                                   '<br>Petrol.MJ: %{y}',
                                   '<br>atGase.MJ: %{z}'),
             color = ~Sector, 
             colors = mycolors,
             marker = list(symbol = 'circle',
                           sizemode = 'diameter'),
             size = ~Total.g.CO2e,
             sizes = c(5,150),
             text = ~paste('<br>Sector:', Sector, 
                           '<br>Description:', Description, 
                           '<br>Sector_sub:', Sector_sub, 
                           '<br>name_sub:', name_sub,
                           '<br>Total.g.CO2e:', Total.g.CO2e)) %>%
  layout(title = 'Total CO2 equivalent vs Energy source (Coal, Petrol, NatGase) <br> by NAICS 2002 Sectors',
         scene = list(xaxis = list(title = 'Coal.MJ',
                                   gridcolor = 'rgb(255, 255, 255)',
                                   type = 'log'),
                      yaxis = list(title = 'Petrol.MJ',
                                   gridcolor = 'rgb(255, 255, 255)',
                                   type = 'log'),
                      zaxis = list(title = 'NatGase.MJ',
                                   gridcolor = 'rgb(255, 255, 255)',
                                   type = 'log')),
         paper_bgcolor = 'rgb(243, 243, 243)',
         plot_bgcolor = 'rgb(243, 243, 243)',
         annotations = list(x = 1.07,
                            y = 1.015,
                            text = 'Sector by NAICS 2002',
                            showarrow = FALSE)); p
```


```{r plotly_GHG_TotalCO2, eval=FALSE, include=FALSE}
htmlwidgets::saveWidget(ggplotly(p),
                        file ="plotly_GHG_TotalCO2.html")
```

After the log transformation, let's take a look at the visulization of a 5-dimemtional plot: Total CO2 Equvivalent as the target variable by Sectors, three resources as input variables as example (click [Here](https://weiquanluo.github.io/img/plotly_GHG_TotalCO2.html)).

# Regression

We are going to natural log transform both X and Y and fit regression models to each of 20 environmental impact variables. The six input resouce vairables consist of Coal.MJ, NatGase.MJ, Petrol.MJ, Bio.Waste.MJ, NonFossElec.MJ, and Water.Withdrawals.Kgal. The environmental impact variables are 7 variables for conventional air polutant, 5 variables for greenhouse gases, 8 variables for Toxic. Druding the data processing, if the count of datapoint for a target variable is less than 100, then we abandon the corresponding models. After data processing and stepwise model selections, it result 16 model candidates, where one for each valid target variable. The following user-defined function are prepared for functional programming with using purrr style lamda function.

## User-defined Functions
```{r udfunc}
# test: target_nm = "CO.g",  X = resource
makedata_map <- function(target_nm, dat, X){
  # loglog transform
  y <- dat %>% select(target_nm)
  Lny <- log(y) %>% stats::setNames(paste0("Ln", names(.)))
  LnX <- log(X+1) %>% stats::setNames(paste0("Ln", names(.)))
  # combine, filter log(y)=0; add ID:Sector
  Xy <- cbind(LnX, Lny)
  Xy <- cbind(dat %>% select(Sector, Description, name_sub, Sector_sub), Xy) 
  Xy <- Xy[!is.infinite(rowSums(Lny)),]
  colnames(Xy) <- colnames(Xy) %>% stringr::str_replace_all("\\.","") 
  return(Xy)
}
fit_model <- function(data){
  best_model <- bestglm::bestglm(Xy=data %>% 
                                   select_if(is.numeric), 
                                 family = gaussian, 
                                 method = "exhaustive", 
                                 IC = "AIC", 
                                 TopModels = 1)
  return(best_model[[1]])
}
bind_coef_star <- function(x) {
  if (stringr::str_detect(x[2] , "\\*")) {
    paste0(x[1], "(",x[2], ")")
  } else if (!is.na(x[1])){
    paste0(x[1])
  } else{
    ""
  }
}
# test: model <- bestglm_list$best_model[[1]], null <- 1
waldtest_map <- function(model, null= NULL){
  test.terms <- paste0("~", names(coef(model))[-1] %>% paste(collapse = "+")) %>% 
    as.formula()
  test_result <- survey::regTermTest(model = model, test.terms ,null = null)
  pval_wald <- test_result[['p']] %>% as.numeric()
  return(pval_wald)
}
```

## LogLog Linear Regression

```{r glm_fit}
# create a dataframe with a column with impact variable names 
target_list <- tibble(target = c(colnames(CPA),colnames(GHG),colnames(TOX))); target_list %>% flatten() %>% unlist()

# piping: variale selection, anova, extract statistic, wald test
bestglm_list <- target_list %>% 
  mutate(data = target %>% 
           map(function(target_nm) makedata_map(target_nm,
                                                dat= dat, 
                                                X = resource))) %>% 
  mutate(rowdata = data %>% map_dbl(nrow)) %>% 
  filter(rowdata > 100) %>% 
  select(-rowdata) %>% 
  mutate(best_model = data %>% map(fit_model)) %>% 
  mutate(anv = best_model %>% map(anova)) %>% 
  mutate(statisics = best_model %>% purrr::map(.f = function(m) broom::glance(m))) %>% 
  tidyr::unnest(statisics) %>% 
  mutate(wald_pval = best_model %>% 
           purrr::map_dbl(function(model) waldtest_map(model= model, null= 1))) %>% 
  mutate(nrows_data = data %>% purrr::map_dbl(nrow)) %>% 
  arrange(desc(adj.r.squared))
# extract coef from each model
coef_list <- bestglm_list %>% 
  mutate(coefs = best_model %>% purrr::map(.f=broom::tidy)) %>% 
  select(target, coefs) %>% 
  tidyr::unnest(coefs) %>% 
  select(target, term, estimate) %>% 
  tidyr::spread(key= term, value = estimate)
# extract pval for all coef from each model
signif_list <- bestglm_list %>% 
  mutate(coefs = best_model %>% purrr::map(.f=broom::tidy)) %>% 
  select(target, coefs) %>% 
  tidyr::unnest(coefs) %>% 
  select(target, term, p.value) %>% 
  tidyr::spread(key= term, value = p.value)
# combind coef and pval for visual
coef_signif_list <- coef_list %>% 
  select(target) %>% 
  cbind(apply(abind::abind(coef_list %>% 
                             select(-target) %>% 
                             mutate_if(is.numeric, signif, digits = 3) %>% 
                             mutate_all(as.character),
                           signif_list %>% 
                             select(-target) %>% 
                             mutate_if(is.numeric, gtools::stars.pval),along=3),
              1:2, bind_coef_star))
# add statistic to the coef and pval
coef_signif_list <- bestglm_list %>% 
  select(target, adj.r.squared, p.value, wald_pval) %>% 
  mutate_at(c("adj.r.squared", "p.value", "wald_pval"), signif, digits = 3) %>% 
  left_join(coef_signif_list, by="target")
# get exponent_sum
coef_signif_list$exponent_sum <- coef_list[,3:8] %>% rowSums(na.rm = TRUE) %>% signif(digits = 3)
```

```{r coef_signif_list}
# result
coef_signif_list %>% 
  arrange(desc(adj.r.squared)) %>% 
  knitr::kable(format = "markdown") 
```

The following linear model with R^2 >0.75

```{r goodmodel}
good_lm <- bestglm_list %>% filter(adj.r.squared >0.75)
good_lm %>% arrange(desc(adj.r.squared)) %>% select(target) %>% flatten() %>% unlist
```

## Diagnosis for GHG CO2 Equvivalent model:

```{r diagsis}
par(mfrow=c(2,3))
plot(good_lm$best_model[[2]], which=1:6)
```

## Partial-residual plots

```{r prplot}
car::crPlots(bestglm_list$best_model[[2]])
```

## Random effect: Impact among sectors

```{r}
# CO2.Fossil.t.CO2e	C: Emissions of Carbon Dioxide (CO2) into the air from each sector from fossil fuel combustion sources. t CO2e = metric tons of CO2 equivalent.
par(mfrow=c(1,2))
i=2
good_lm$data[[i]][,ncol(good_lm$data[[i]])] %>% boxplot()
plot(good_lm$data[[i]][,1], good_lm$data[[i]][,ncol(good_lm$data[[i]])])
```

```{r}
# random effect model
data_anv <- good_lm$data[[2]] %>% select(-Description,-name_sub, -Sector_sub)
model.rand_block <- nlme::lme(data = data_anv,
                     LnTotalgCO2e ~ LnBioWasteMJ + LnCoalMJ + LnNatGaseMJ + LnNonFossElecMJ + LnPetrolMJ,
                     random = ~1|Sector)
# anova(model.rand_block)
# fixed effect model
model.fixed = nlme::gls(data = data_anv,
                  LnTotalgCO2e ~ LnBioWasteMJ + LnCoalMJ + LnNatGaseMJ + LnNonFossElecMJ + LnPetrolMJ,
                  method="REML")
# anova(model.fixed)
# Test the random effects in the model
anova(model.rand_block, model.fixed)
```

The extremely small p-value in testing random effect suggest that there are significant difference in green house gases CO2 equivalence among sectors.

# Clustering for CO2 Equvivalent

For instance, we are interested in what the outlier industries in producing CO2 Equvivalent. In general, we need to be extremely careful to rescaling data for distance based modeling that is sensitive to the distance between datapoints. From the above analysis, we understand this data is suitable in log scale. With considerations, we maintain the log transformation in the following distance based modeling, for reason of consistency and explanatory power. At this section, we are going to cluster the the relative size (or magnitude) of Greenhouse gases CO2 Equvivalent among all industries, instead of the scalar quantity. We used a DBSCANS algorithm to cluster the industries with magnitude of green house gases CO2 equivalence and the magnitude of the six input sources variable. The clearn log-transformed data of fitting CO2 Equvivalent linear regression is obtained from the `bestglm_list` object above. Then, we feed this log-transformed data to the cluster algorithm in python.

```{r dat_ghg}
# input data to clustering using python
dat_ghg_total_co2e <-bestglm_list %>% filter(target =="Total.g.CO2e") %>% select(data)
dat_ghg_total_co2e <- dat_ghg_total_co2e[[1]][[1]]
dat_ghg <- dat_ghg_total_co2e[,-c(1:4)]
dat_ghg %>% head %>% knitr::kable(format = "markdown") 
```

```{r use_python}
# use python3 engine
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python package}
# data type
import pandas as pd
import numpy as np
# visualization
import seaborn as sns
import matplotlib.pyplot as plt
import collections
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
# fit model
from sklearn.cluster import DBSCAN
```

```{python getX}
# input
dat_ghg = r.dat_ghg
dat_ghg1 = dat_ghg.iloc[:,1:]
```

## Dendrogram
```{python dendrogram}
# dendrogram
Z = linkage(dat_ghg1, method='ward')
plt.figure(figsize=(15,15))
dendrogram(Z) 
plt.yscale('symlog')
plt.show()
```

## Elbow plot

```{python elbow}
# Elbow plot: number of instance in cluster by hyperparameter eps
count = []
for i in np.arange(0.01, 5, 0.01): 
    clustering = DBSCAN(eps= i).fit(dat_ghg1)
    a = clustering.labels_
    b = collections.Counter(a).get(-1)
    count.append(b)
plt.plot(pd.Series(np.arange(0.01, 5, 0.01)),count)
```

## DBSCAN at eps = 4

```{python fit model}
Cluster_ghg = DBSCAN(eps=4).fit(dat_ghg1)
cluster_labels = Cluster_ghg.labels_
```

## Visualize clusters

```{r cluster_point_plot}
dat_ghg_total_co2e$Sector <- dat_ghg_total_co2e$Description
dat_ghg_total_co2e$labels <- py$cluster_labels %>% as.factor()
p <- ggplot(data = dat_ghg_total_co2e, 
            aes(text = paste("sub_Sector:", name_sub))) + 
  geom_point(aes(x = Sector, 
                 y = LnTotalgCO2e, 
                 color = labels)) +
  coord_flip() + 
  theme(text = element_text(size=8)); p
```

```{r cluster_point_plot_ly, include=FALSE}
htmlwidgets::saveWidget(ggplotly(p),
                        file ="ghg_total_co2e_cluster.html")
```

(click [HERE](https://weiquanluo.github.io/img/ghg_total_co2e_cluster.html) for interactive plot)

# Discussion

# Final Note

The challenges of this project:

* Manage data multicollinearity, heteroskedastic, highly right skewed.
* Piping data processing, functional programming, statistic extraction
* Communication between data processing in R and machine learning in python


# Exercise

Please, click [HERE]() to Exercise.
